# Введение в теорию глубокого обучения


*Empirical risk and its approximation - Basic loss functions. Its evolution based the problem of face recognition - Theoretical justification of adversarial learning methods - Adversarial examples and defense against them - Certified Robustness - Variational Inference - AE, VAE and CVAE - Markov Chain Monte Carlo - Diffusion Model - Limiting theorems for the training process*

[**Вводная информация о курсе на русском языке.**](https://docs.google.com/document/d/1_bk6THgWMIPGQMGir7pNfUxiOqf48azNSHY3YChTVRo/edit?usp=sharing)

---

# **#Introduction to Deep Learning Theory**

## **#Course Resources**

### **#Time and Zoom**

- **Course starts:** 6 February, 2024
- **Time:** Tuesdays, 18:30, ''online''

### **#Updates and Feedback**


- Course e-mail: papermsucode@gmail.com

### **#Materials**

- Links to lectures:
    - 2024.02.06. Empirical Risk and its Approximation. Loss Function. (Stochastic) Gradient Descent. MLE and MAP. KL-divergence and Cross Entropy
        - 
            
            [pdf**MIPT_lecture01-ER_Loss.pdf**1.20 MB](https://wiki.cogmodel.mipt.ru/api/attachments.redirect?id=e9f3bb70-d6b7-49e6-afdf-39fd5117704f)
            
        - [**Video**](https://www.youtube.com/watch?v=5EJsCk_O6So)
    - 2024.02.13. Task of Representation Learning. FaceID. Evolution of Loss Function. SoftMax, Contrastive and Angular-based Losses
        - 
            
            [pdf**MIPT_lecture02-FaceID_Loss.pdf**4.41 MB](https://wiki.cogmodel.mipt.ru/api/attachments.redirect?id=57757914-948f-4500-861c-edd59da867e5)
            
        - [**Video**](https://www.youtube.com/watch?v=hlu_s7Znj1Q)
    - 2024.02.20. Generative Adversarial Networks
        - 
            
            [pdf**MIPT_lecture03-GAN.pdf**8.75 MB](https://wiki.cogmodel.mipt.ru/api/attachments.redirect?id=8f18bda7-68ca-4da9-b19d-db9be6555d3d)
            
        - [**Video**](https://www.youtube.com/watch?v=rbqr8q9lSGQ)
    - 2024.02.27. Bayesian Inference, Bayesian Neural Network, Variational Inference, Autoencoder, Variational Autoencoder, Conditional Variational Autoencoder
        - 
            
            [pdf**MIPT_lecture04-VI_AE_VAE_CVAE.pdf**1.65 MB](https://wiki.cogmodel.mipt.ru/api/attachments.redirect?id=8895da2d-dbc0-4b05-a1cd-2d3e93e84f5f)
            
        - [**Video**](https://www.youtube.com/watch?v=Wf-Hm0SzP5s) (MM)
    - 2024.03.05. Recap of Markov Chains. Markov Chain Monte Carlo. Gibbs sampler. Metropolis-Hastings sampler. Langevin dynamics and Metropolis-Adjusted Langevin. Stochastic Gradient Langevin Dynamics
        - 
            
            [pdf**MIPT_lecture05-MCMC.pdf**814.13 kB](https://wiki.cogmodel.mipt.ru/api/attachments.redirect?id=ff95a51a-b361-4cd2-a8be-66d0fcbf1f13)
            
        - [**Video**](https://www.youtube.com/watch?v=m1RYH6Qpnqw)
    - 2024.03.12. Recap of Variational Autoencoder. Markovian Hierarchical VAE. Diffusion models: Variational Diffusion Models, Diffusion Denoising Probabilistic Models, Diffusion Denoising Implicit Models, Classifier and classifier-free guidance, 3 interpretations
        - 
            
            [pdf**MIPT_lecture06-Diffusion.pdf**1.65 MB](https://wiki.cogmodel.mipt.ru/api/attachments.redirect?id=42231e4b-bcc2-4e9d-9644-c1b8aee95c96)
            
        - [**Video**](https://www.youtube.com/watch?v=Fer6bEnB_DA)
    - 2024.03.19. Adversarial Robustness I: Great Success of CNNs, Robustness Phenomenon, Taxonomy of Adversarial Attacks, l_p norms, Digital Domain, Fast Gradient Sign Method and its variants, Universal Attacks, Top-k Attacks, l_0 attacks
        - 
            
            [pdf**MIPT_lecture07-advrob_I.pdf**9.93 MB](https://wiki.cogmodel.mipt.ru/api/attachments.redirect?id=59e2620b-1e49-429f-8986-35978c9df871)
            
        - [**Video**](https://www.youtube.com/watch?v=iWjErgoxJuo) (MM)
    - 2024.03.26. Adversarial Robustness II: Adversarial examples in real world, Adversarial attack on Face detection and Face ID systems, Defense from adversarial examples in real world, Black-box Face restoration
        - 
            
            [pdf**MIPT_lecture-AdvRob_II_Real.pdf**20.05 MB](https://wiki.cogmodel.mipt.ru/api/attachments.redirect?id=d7814bee-312a-4104-8717-94cc1b88abb6)
            
        - [**Video**](https://www.youtube.com/watch?v=G0aE3B9DVlM)
    - 2024.04.02. Certified Robustness I: definitions of Certified Robustness, connection to Lipschitzness, Randomized Smoothing and its variants
        - 
            
            [pdf**MIPT_lecture09-CertRob_I_RS.pdf**968.27 kB](https://wiki.cogmodel.mipt.ru/api/attachments.redirect?id=8f8a2b31-aa73-492a-a38d-e1df70fd5d91)
            
        - [**Video**](https://www.youtube.com/watch?v=oDplNIhZCOA)
    - 2024.04.09. Certified Robustness II: recap of Certified Robustness, Ablations on base classifier/norm of perturbation/smoothing distribution, Certification in High Dimensional case, Certification of Semantic Perturbations, Application to different Computer Vision tasks
        - 
            
            [pdf**MIPT_lecture10-CertRob_II_HighDim.pdf**10.04 MB](https://wiki.cogmodel.mipt.ru/api/attachments.redirect?id=88c56ac7-021c-472a-81aa-cc1eeeaa03a5)
            
        - [**Video**](https://www.youtube.com/watch?v=SB4pXmdMVVk)
    - 2024.04.16. Neural Tangent Kernel: Lazy regime of training, GD as PDE, NTK and CNTK, NTK convergence rates
        - 
            
            [pdf**MIPT_lecture11-NTK.pdf**1.32 MB](https://wiki.cogmodel.mipt.ru/api/attachments.redirect?id=f48e4263-a7ce-4a20-ad0b-d984520318b0)
            
        - [**Video**](https://www.youtube.com/watch?v=Av-21QOLp2k)

### **#Staff**

- Lector: [**Aleksandr Petiushko**](https://wiki.cogmodel.mipt.ru/doc/petyushko-aleksandr-aleksandrovich-L0X6yhAylZ), PhD

## **#Overview**

Deep learning is a young (approximate date of origin is 2011-2012), but actively developing area of machine learning, which is characterized primarily by the use of neural networks with a large (hence the word “''deep''” in the name) number of layers in their architecture. Initially, deep learning was a predominantly empirical field of knowledge in which new findings were primarily found experimentally. Subsequently, many findings began to receive theoretical justification, and somewhere theory is now even ahead of practice. The course will cover the basic concepts that are used in the theoretical consideration of empirical methods of deep learning - the theory of loss functions, working with data distributions, adversarial learning, stability of neural networks, and limit theorems.

## **#Course Content**

- Empirical risk and its approximation
    - The basic concepts of measuring the quality of the work of a machine learning algorithm are empirical risk and its approximation. Differentiability. Stochastic gradient descent. Regularization. Probabilistic meaning of loss functions on the example of maximum likelihood and a posteriori probability.
- Basic loss functions. Its evolution based the problem of face recognition
    - The main classification functions of losses are logistic, cross entropy. Entropy and the Gibbs inequality. Functions on distributions. Kullback-Leibler distance. Evolution of loss functions on the example of the problem of face recognition.
- Theoretical justification of adversarial learning methods
    - The mechanism of adversarial learning as a minimax. Derivation of formulas reflecting a practical approach to training. Connection with the Wasserstein metric.
- Adversarial examples and defense against them
    - The surprising effect of the instability of neural networks to the input perturbations. Examples of adversarial perturbations. Basic methods for constructing adversarial examples and defending against them. Classification of adversarial examples. Adversarial Examples implementable in the Real-World.
- Certified Robustness
    - The concepts of certificate and certified robustness. Classical approach using randomized smoothing. Neyman-Pearson Lemma. Curse of dimensionality in computer vision problems.
- Variational Inference
    - Bayes' theorem and posterior probability. Approximation using a parametric family of distributions. Lower bound by ELBO.
- AE, VAE and CVAE
    - Concepts of autoencoder, variational autoencoder, conditional variational autoencoder and their differences. Architectural implementation in practice.
- Markov Chain Monte Carlo
    - The problem of sampling from an empirical space in a (high) multidimensional space. Gibbs, Metropolis and Metropolis-Hastings samplers. Relationship with Langevin dynamics.
- Diffusion Models
    - Forward and reverse process as an analogue of the diffusion process. Derivation of formulas and architectural implementation in practice.
- Limiting theorems for the training process
    - Limiting (existence) theorems for approximation, as well as the dynamics of the convergence of the training process.

## **#Projects Proposals**

1. Investigate Neural Collapse on different datasets (MNIST, Omniglot, LFW, ...)
2. Make a comparison study of angular-based losses vs metric-based ones on different datasets (MNIST, Omniglot, LFW, ...)
3. Think of evaluation metric for GAN solution (aside from [**Inception Score**](https://en.wikipedia.org/wiki/Inception_score) / [**Frechet Inception Distance**](https://en.wikipedia.org/wiki/Fr%C3%A9chet_inception_distance)) and make a comparison study of this metric for different GAN solution: vanilla GAN, WGAN, WGAN-GP
4. Implement and analyze the BNN recognition results using different priors for weights (Uniform, Gaussian, Laplace) on different datasets (MNIST, Omniglot, LFW, ...)
    1. Do it with Variational Inference
    2. Do it with MCMC
5. Explore the Diffusion generation quality vs number of steps on different datasets (MNIST, Omniglot, LFW, ...)
    1. Do it with unconditional generation
    2. Do it with classifier(-free) guidance
    3. Explore different strategies of α (β) decrease schedule
6. Make a quantitave and qualitative analysis of different l0/l1/l2/l∞-based Adversarial Attacks (success rate, number of iterations, etc) on different datasets (MNIST, Omniglot, LFW, ...)
    1. Do it for the Universal Adversarial Attack as well
    2. Compare the transferability for different NN architectures (LeNet, VGG, ResNet, etc)
7. Create a real-world attack demo for any detection/recognition system

## **#Additional Resources**

### **#Bibliography**

- [**Machine Learning Lecture Course**](http://www.machinelearning.ru/wiki/index.php?title=%D0%9C%D0%B0%D1%88%D0%B8%D0%BD%D0%BD%D0%BE%D0%B5_%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5_%28%D0%BA%D1%83%D1%80%D1%81_%D0%BB%D0%B5%D0%BA%D1%86%D0%B8%D0%B9%2C_%D0%9A.%D0%92.%D0%92%D0%BE%D1%80%D0%BE%D0%BD%D1%86%D0%BE%D0%B2%29) on [**http://www.machinelearning.ru**](http://www.machinelearning.ru/) from Vorontsov K.V.
- Hastie, T. and Tibshirani, R. and Friedman, J. [**The Elements of Statistical Learning**](https://web.stanford.edu/~hastie/ElemStatLearn/printings/ESLII_print12.pdf), 2nd edition, Springer, 2009.
- Bishop, C.M. [**Pattern Recognition and Machine Learning**](https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf), Springer, 2006.
- Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep learning. Vol. 1. Cambridge: MIT press, 2016.
- Matus Telgarsky, Deep learning theory lecture [**notes**](https://mjt.cs.illinois.edu/dlt/index.pdf), 2021
- Sanjeev Arora et al., Theory of Deep learning book [**draft**](https://www.dropbox.com/s/smkp4vasbiszhw4/DLbook.pdf?dl=0), 2020

### **#Introduction to machine learning**

- Homemade Machine Learning: [**github repo**](https://github.com/trekhleb/homemade-machine-learning)
- Machine learning: [**Course**](https://www.coursera.org/learn/machine-learning) by Andrew Ng on the site [**https://www.coursera.org**](https://www.coursera.org/)

### **#Theoretic Courses**

- Foundations of Deep Learning: [**Course**](https://uclaml.github.io/CS269-Spring2021/) at UCLA
- Deep learning theory: [**Course**](https://mjt.cs.illinois.edu/dlt/) at UIUC
- Theoretical Deep Learning: [**Course**](https://www.cs.princeton.edu/courses/archive/fall19/cos597B/) at Princeton
