{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## МТИИ 2021: Python\n",
    "## Семинар 2:\n",
    "## Часть 1: Краткое руководство по написанию кода на Python (Python Enhanced Proposal - PEP8). \n",
    "## Часть 2: Регулярные выражения. Токенизация. Лемматизация. Стемминг. Модель биграмм.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Часть 1:\n",
    "\n",
    "PEP8 можно считать набором правил для написания кода на Python. Данный документы покрывает следующие пункты:\n",
    "\n",
    "* Introduction\n",
    "* A Foolish Consistency is the Hobgoblin of Little Minds\n",
    "* Code Lay-out\n",
    "    * Indentation\n",
    "    * Tabs or Spaces?\n",
    "    * Maximum Line Length\n",
    "    * Should a Line Break Before or After a Binary Operator?\n",
    "    * Blank Lines\n",
    "    * Source File Encoding\n",
    "    * Imports\n",
    "    * Module Level Dunder Names\n",
    "* String Quotes\n",
    "* Whitespace in Expressions and Statements\n",
    "    * Pet Peeves\n",
    "    * Other Recommendations\n",
    "* When to Use Trailing Commas\n",
    "* Comments\n",
    "    * Block Comments\n",
    "    * Inline Comments\n",
    "    * Documentation Strings\n",
    "* Naming Conventions\n",
    "    * Overriding Principle\n",
    "    * Descriptive: Naming Styles\n",
    "    * Prescriptive: Naming Conventions\n",
    "        * Names to Avoid\n",
    "        * ASCII Compatibility\n",
    "        * Package and Module Names\n",
    "        * Class Names\n",
    "        * Type Variable Names\n",
    "        * Exception Names\n",
    "        * Global Variable Names\n",
    "        * Function and Variable Names\n",
    "        * Function and Method Arguments\n",
    "        * Method Names and Instance Variables\n",
    "        * Constants\n",
    "        * Designing for Inheritance\n",
    "    * Public and Internal Interfaces\n",
    "* Programming Recommendations\n",
    "    * Function Annotations\n",
    "    * Variable Annotations\n",
    "\n",
    "### Вопрос 1: Какой из двух вариантов написания кода правильный? Почему?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'long_function_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Вариант 1\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m foo \u001b[38;5;241m=\u001b[39m long_function_name(var_one, var_two,\n\u001b[1;32m      3\u001b[0m                          var_three, var_four)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlong_function_name\u001b[39m(\n\u001b[1;32m      6\u001b[0m         var_one, \n\u001b[1;32m      7\u001b[0m         var_two, \n\u001b[1;32m      8\u001b[0m         var_three,\n\u001b[1;32m      9\u001b[0m         var_four):\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(var_one)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'long_function_name' is not defined"
     ]
    }
   ],
   "source": [
    "# Вариант 1\n",
    "foo = long_function_name(var_one, var_two,\n",
    "                         var_three, var_four)\n",
    "\n",
    "def long_function_name(\n",
    "        var_one, \n",
    "        var_two, \n",
    "        var_three,\n",
    "        var_four):\n",
    "    print(var_one)\n",
    "# 4 пробела - хороший тон\n",
    "\n",
    "foo = long_function_name(\n",
    "    var_one, var_two,\n",
    "    var_three, var_four)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вариант 2\n",
    "foo = long_function_name(var_one, var_two,\n",
    "    var_three, var_four)\n",
    "\n",
    "def long_function_name(\n",
    "    var_one, var_two, var_three,\n",
    "    var_four):\n",
    "    print(var_one)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вопрос 2: Что лучше использовать для отсутпов, табуляцию или пробелы?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Табуляцию - 4 пробела на любой системе, нельзя всё перемешивать"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вопрос 3: Нужно ли вводить ограничение на максимальную длину строки? Почему?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Конвертация в пдф тетрадки осложняется   \n",
    "Просматриваемость кода уменьшается, блоки не будут видны   \n",
    "Ограничение 80 символов   \n",
    "В старых терминалах было ограничение в 80 символов   \n",
    "\n",
    "### Вопрос 4: Какой из двух вариантов написания кода правильный? Почему?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вариант 1\n",
    "income = (gross_wages +\n",
    "          taxable_interest +\n",
    "          (dividends - qualified_dividends) -\n",
    "          ira_deduction -\n",
    "          student_loan_interest)\n",
    "# старый вариант"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вариант 2\n",
    "income = (gross_wages\n",
    "          + taxable_interest\n",
    "          + (dividends - qualified_dividends)\n",
    "          - ira_deduction\n",
    "          - student_loan_interest)\n",
    "# новый вариант"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вопрос 5: Какую кодировку нужно использовать в python3? Почему всегда utf-8?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# чтобы можно было писать комменты на кириллице\n",
    "# в идеале комменты на английском\n",
    "# вопрос распространённости кодировки\n",
    "# utf8 - всегда один ваприант"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вопрос 6: Какие из трех  вариантов написания кода являются правильными? Почему?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# вариант 1\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# вариант 2 - неправильный, импортируем что-то из чего-то, \n",
    "# импортируем всё в одном месте, неудобно удалять, ненаглядно\n",
    "import os, sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# вариант 3\n",
    "from subprocess import Popen, PIPE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вопрос 7: Какой тип ковыче нужно использовать для задания строк (одинарный или двойной)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \"123\"  584\n"
     ]
    }
   ],
   "source": [
    "s1 = ' \"123\" '\n",
    "s2 = \"584\"\n",
    "print(s1, s2) # можно любой использовать, неважно какой вид использовать "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вопрос 8: Использование пробелов. Выберите правильные варианты:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# вариант 1 - correct\n",
    "spam(ham[1], {eggs: 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# вариант 2 \n",
    "spam( ham[ 1 ], { eggs: 2 } )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# вариант 1 -- correct\n",
    "foo = (0,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# вариант 2 but, it is more convenient to enter, tabulation can fit it\n",
    "bar = (0, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# вариант 1 \n",
    "ham[1:9], ham[1:9:3], ham[:9:3], ham[1::3], ham[1:9:]\n",
    "ham[lower:upper], ham[lower:upper:], ham[lower::step]\n",
    "ham[lower+offset : upper+offset]\n",
    "ham[: upper_fn(x) : step_fn(x)], ham[:: step_fn(x)]\n",
    "ham[lower + offset : upper + offset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# вариант 2 - correct\n",
    "ham[lower + offset:upper + offset]\n",
    "ham[1: 9], ham[1 :9], ham[1:9 :3]\n",
    "ham[lower : : upper]\n",
    "ham[ : upper]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expected ':' (1686236772.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    if f is True #not f == True -- является\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m expected ':'\n"
     ]
    }
   ],
   "source": [
    "f = []\n",
    "if f is True #not f == True -- является\n",
    "if not F:\n",
    "and F:\n",
    "# не нужно сравнивать с True and False\n",
    "# сравнивать объекты одного типа\n",
    "_ = int(input()) # всё, что нам не нужно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(map(int, ['1', '2']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "5\n",
    "1 2 4 3 3\n",
    "\n",
    "{1: 5, 2:6, 3:7, 4:2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы рассмотрели лишь часть руководства PEP8, более подробно вы можете изучить его самостоятельно в [документации](https://www.python.org/dev/peps/pep-0008/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Часть 2:\n",
    "\n",
    "## 1. Регулярные выражения\n",
    "\n",
    "Регулярные выражения (regular expressions, RegExp) —\n",
    "это формальный язык для операций с подстроками.\n",
    "\n",
    "Чаще всего регулярные выражения используются для:\n",
    "* поиска в строке;\n",
    "* разбиения строки на подстроки;\n",
    "* замены части строки;\n",
    "* валидации (проверки).\n",
    "\n",
    "## Синтаксис:\n",
    "<img src='https://raw.githubusercontent.com/hse-ds/iad-applied-ds/master/2020/seminars/seminar11/r.png'>\n",
    "\n",
    "\n",
    "\n",
    "## Онлайн упражнения\n",
    "https://regexone.com/\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Регулярные выражения в Python\n",
    "\n",
    "Основные методы:\n",
    "```\n",
    "• re.match() — поиск совпадения в начале строки\n",
    "• re.search() — поиск первого совпадения\n",
    "• re.findall() — поиск всех совпадений (возвр. список)\n",
    "• re.split() — разбиение строки\n",
    "• re.sub() — замена подстроки\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## match\n",
    "ищет по заданному шаблону в начале строки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 4), match='abcd'>\n"
     ]
    }
   ],
   "source": [
    "result = re.match('ab+c.', 'abcdefghijkabcabcd') # ищем по шаблону 'ab+c.' \n",
    "print (result) # совпадение найдено:\n",
    "result_  = re.match('ab+k', 'abcdefghijkabcabcd')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abcd\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'group'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(result\u001b[38;5;241m.\u001b[39mgroup(\u001b[38;5;241m0\u001b[39m)) \u001b[38;5;66;03m# выволмс найденное совпадение\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(result_\u001b[38;5;241m.\u001b[39mgroup(\u001b[38;5;241m0\u001b[39m))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'group'"
     ]
    }
   ],
   "source": [
    "print(result.group(0)) # выволмс найденное совпадение\n",
    "print(result_.group(0)) # выволмс найденное совпадение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "result = re.match('abc.', 'abdefghijkabcabc')\n",
    "print(result) # совпадение не найдено"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1:\n",
    "Проверьте, начинаются ли строки c заглавной буквы и если да, то вывести эту заглавную букву. Придумайте свои примеры строк для проверки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[A-ZА-ЯЁ]\n",
      "Строка: 'Hello world' начинается с заглавной буквы 'H'\n",
      "Строка: 'привет мир' не начинается с заглавной буквы\n",
      "Строка: 'Python is cool' начинается с заглавной буквы 'P'\n",
      "Строка: '123start' не начинается с заглавной буквы\n",
      "Строка: 'Good morning' начинается с заглавной буквы 'G'\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "strings = ['Hello world', 'привет мир', 'Python is cool', '123start', 'Good morning'] \n",
    "\n",
    "pattern = r'[A-ZА-ЯЁ]'\n",
    "print(pattern)\n",
    "\n",
    "for string in strings:\n",
    "    result = re.match(pattern, string)\n",
    "    if result:\n",
    "        print(f\"Строка: '{string}' начинается с заглавной буквы '{result.group(0)}'\")\n",
    "    else:\n",
    "        print(f\"Строка: '{string}' не начинается с заглавной буквы\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## search\n",
    "ищет по всей строке, возвращает только первое найденное совпадение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ab\n"
     ]
    }
   ],
   "source": [
    "strings = 'abcHelloabc, приветмир, abcPythonabc, start123, Goodmorning, Мирпрекрасен, Ёжиквтумане'\n",
    "pattern = 'ab'\n",
    "#pattern = r'[a-aа-яё]'\n",
    "result = re.search(pattern, strings)\n",
    "print(result.group(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2\n",
    "Проверьте, есть ли в строке вопросительный знак. Придумайте свои примеры для проверки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Примеры строк для проверки\n",
    "strings = [\n",
    "    'Как дела?', \n",
    "    'This is a test string.', \n",
    "    'What time is it?', \n",
    "    'No questions here.', \n",
    "    'Погода хорошая сегодня', \n",
    "    'Вы придете завтра?'\n",
    "]\n",
    "\n",
    "pattern = r'\\?' # not r'?' - Мы экранируем вопросительный знак, чтобы регулярное выражение \n",
    "# воспринимало его как обычный символ, а не специальный метасимвол.\n",
    "\n",
    "for string in strings:\n",
    "    if re.search(pattern, string):\n",
    "        print(True);\n",
    "    else:\n",
    "        print(False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## findall\n",
    "возвращает список всех найденных совпадений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abcd', 'abca']\n"
     ]
    }
   ],
   "source": [
    "result = re.findall('ab+c.', 'abcdefghijkabcabcxabc') \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вопросы: \n",
    "1) почему нет последнего abc?\n",
    "2) почему нет abcx?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 3\n",
    "Вернуть список первых двух букв каждого слова в строке, состоящей из нескольких слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Я', 'ты', 'мы', 'вы', 'он', 'Сл', 'Не', 'ду']\n"
     ]
    }
   ],
   "source": [
    "str_ = \"Я, ты, мы, вы, они. Случайность? Не думаю!\"\n",
    "pattern = r'\\b\\w{1,2}' # \\b — граница слова. \\w{2} — две подряд идущие буквенно-цифровые символы (буквы или цифры).\n",
    "# if  pattern = r'\\b\\w{1, 2}' we have zero-list in result\n",
    "result = re.findall(pattern, str_)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split\n",
    "разделяет строку по заданному шаблону\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['itsy', ' bitsy', ' teenie', ' weenie']\n"
     ]
    }
   ],
   "source": [
    "result = re.split(',', 'itsy, bitsy, teenie, weenie') \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "можно указать максимальное количество разбиений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['itsy', ' bitsy', ' teenie, weenie']\n"
     ]
    }
   ],
   "source": [
    "result = re.split(',', 'itsy, bitsy, teenie, weenie', maxsplit = 2) \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 4\n",
    "Разбейте строку, состоящую из нескольких предложений, по точкам, но не более чем на 3 предложения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Нашел он полон двор услуги;\n",
      "К покойнику со всех сторон\n",
      "Съезжались недруги и други,\n",
      "Охотники до похорон\n",
      "Покойника похоронили\n",
      "Попы и гости ели, пили\n",
      "И после важно разошлись,\n",
      "Как будто делом занялись\n",
      "Вот наш Онегин — сельский житель,\n",
      "Заводов, вод, лесов, земель\n",
      "Хозяин полный, а досель\n",
      "Порядка враг и расточитель,\n",
      "И очень рад, что прежний путь\n",
      "Переменил на что-нибудь.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"\"\"\n",
    "Нашел он полон двор услуги;\n",
    "К покойнику со всех сторон\n",
    "Съезжались недруги и други,\n",
    "Охотники до похорон.\n",
    "Покойника похоронили.\n",
    "Попы и гости ели, пили\n",
    "И после важно разошлись,\n",
    "Как будто делом занялись.\n",
    "Вот наш Онегин — сельский житель,\n",
    "Заводов, вод, лесов, земель\n",
    "Хозяин полный, а досель\n",
    "Порядка враг и расточитель,\n",
    "И очень рад, что прежний путь\n",
    "Переменил на что-нибудь.\"\"\"\n",
    "####### Ваш код здесь ########\n",
    "result = re.split(r'\\.', text, maxsplit = 3)\n",
    "\n",
    "#result = [item.replace('\\n', ' ') for item in result]\n",
    "result = [item.strip() for item in result]\n",
    "\n",
    "\n",
    "for res in result:\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'result' is not defined"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sub\n",
    "ищет шаблон в строке и заменяет все совпадения на указанную подстроку\n",
    "\n",
    "параметры: (pattern, repl, string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bbcbbc\n"
     ]
    }
   ],
   "source": [
    "result = re.sub('a', 'b', 'abcabc')\n",
    "print (result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 5:\n",
    "Замените все цифры на звездочки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****dvhfks**********uhgj**l****\n"
     ]
    }
   ],
   "source": [
    "str_ = \"7253dvhfks0340921934uhgj34l4321\"\n",
    "####### Ваш код здесь ########10\n",
    "pattern = r'\\d'\n",
    "#pattern = r'[0-9]' # we use ^ when we are looking for \n",
    "#something at the beginning of a line\n",
    "#result = [item.sub('*', pattern) for item in str_]\n",
    "\n",
    "result = re.sub(pattern, '*', str_)\n",
    "print(result)\n",
    "##############################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compile\n",
    "компилирует регулярное выражение в отдельный объект"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Слова', 'Да', 'больше', 'ещё', 'больше', 'слов', 'Что-то', 'ещё']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Пример: построение списка всех слов строки:\n",
    "prog = re.compile('[А-Яа-яё-]+')\n",
    "prog.findall(\"Слова? Да, больше, ещё больше слов! Что-то ещё.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Слова', 'Да', 'больше', 'ещё', 'больше', 'слов', 'Что-то', 'ещё']\n"
     ]
    }
   ],
   "source": [
    "#without compile\n",
    "\n",
    "import re\n",
    "\n",
    "# Поиск всех слов в строке без компиляции\n",
    "result = re.findall(r'[А-Яа-яё\\-]+', \"Слова? Да, больше, ещё больше слов! Что-то ещё.\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 6\n",
    "Для выбранной строки постройте список слов, которые длиннее трех символов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['пример', 'строки', 'демонстрации', 'кода.']\n"
     ]
    }
   ],
   "source": [
    "str = \"Это пример строки для демонстрации кода.\"\n",
    "\n",
    "words = str.split()  # Разбиваем строку на слова\n",
    "long_word = []\n",
    "\n",
    "for word in words:\n",
    "  if len(word) > 3:\n",
    "    long_word.append(word)\n",
    "\n",
    "print(long_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['пример', 'строки', 'демонстрации', 'кода']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "str = \"Это пример строки для демонстрации кода.\"\n",
    "\n",
    "# Используем регулярное выражение для нахождения слов длиннее трех символов\n",
    "pattern = r'\\b\\w{4,}\\b'\n",
    "#\\b — это граница слова.\n",
    "#\\w{4,} — это шаблон, который находит слова, \n",
    "#состоящие из 4 и более буквенно-цифровых символов.\n",
    "\n",
    "# Находим все слова, которые соответствуют шаблону\n",
    "long_words = re.findall(pattern, str)\n",
    "\n",
    "print(long_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 7\n",
    "Вернуть первое слово строки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Пример\n"
     ]
    }
   ],
   "source": [
    "#^\\w+:\n",
    "# ^ — указывает на начало строки.\n",
    "# \\w+ — соответствует одному или более буквенно-цифровым символам (слово).\n",
    "\n",
    "import re\n",
    "text = \"Пример строки для теста.\"\n",
    "\n",
    "pattern = r'^\\w+'\n",
    "\n",
    "res = re.match(pattern, text)\n",
    "if res:\n",
    "    print(res.group(0))\n",
    "else:\n",
    "    print(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 8\n",
    "Вернуть список доменов (@gmail.com, @rest.biz и т.д.) из списка адресов электронной почты:\n",
    "\n",
    "abc.test@gmail.com, xyz@test.in, test.first@analyticsvidhya.com, first.test@rest.biz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@gmail.com', '@test.in', '@analyticsvidhya.com', '@rest.biz']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Список адресов электронной почты\n",
    "emails = \"abc.test@gmail.com, xyz@test.in, test.first@analyticsvidhya.com, first.test@rest.biz\"\n",
    "\n",
    "# Регулярное выражение для нахождения доменов\n",
    "pattern = r'@[\\w\\.]+'\n",
    "\n",
    "# Поиск доменов в строке\n",
    "domains = re.findall(pattern, emails)\n",
    "\n",
    "# Вывод результата\n",
    "print(domains)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Токенизация\n",
    "\n",
    "\n",
    "Самый наивный способ токенизировать текст -- разделить с помощью split. Но split упускает очень много всего, например, банально не отделяет пунктуацию от слов. Кроме этого, есть ещё много менее тривиальных проблем. Поэтому лучше использовать готовые токенизаторы.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-09-10 13:19:31--  https://raw.githubusercontent.com/hse-ds/iad-applied-ds/master/2020/seminars/seminar11/alice.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 231397 (226K) [text/plain]\n",
      "Saving to: 'alice.txt'\n",
      "\n",
      "alice.txt           100%[===================>] 225.97K  --.-KB/s    in 0.09s   \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2024-09-10 13:19:32 (2.36 MB/s) - 'alice.txt' saved [231397/231397]\n",
      "\n",
      "--2024-09-10 13:19:32--  https://raw.githubusercontent.com/hse-ds/iad-applied-ds/master/2020/seminars/seminar11/dinos.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 19910 (19K) [text/plain]\n",
      "Saving to: 'dinos.txt'\n",
      "\n",
      "dinos.txt           100%[===================>]  19.44K  --.-KB/s    in 0.002s  \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2024-09-10 13:19:32 (9.83 MB/s) - 'dinos.txt' saved [19910/19910]\n",
      "\n",
      "Requirement already satisfied: nltk in /home/angelika/anaconda3/lib/python3.12/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /home/angelika/anaconda3/lib/python3.12/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/angelika/anaconda3/lib/python3.12/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/angelika/anaconda3/lib/python3.12/site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in /home/angelika/anaconda3/lib/python3.12/site-packages (from nltk) (4.66.4)\n",
      "Collecting pymystem3==0.1.10\n",
      "  Downloading pymystem3-0.1.10-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: requests in /home/angelika/anaconda3/lib/python3.12/site-packages (from pymystem3==0.1.10) (2.32.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/angelika/anaconda3/lib/python3.12/site-packages (from requests->pymystem3==0.1.10) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/angelika/anaconda3/lib/python3.12/site-packages (from requests->pymystem3==0.1.10) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/angelika/anaconda3/lib/python3.12/site-packages (from requests->pymystem3==0.1.10) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/angelika/anaconda3/lib/python3.12/site-packages (from requests->pymystem3==0.1.10) (2024.6.2)\n",
      "Downloading pymystem3-0.1.10-py3-none-any.whl (10 kB)\n",
      "Installing collected packages: pymystem3\n",
      "Successfully installed pymystem3-0.1.10\n",
      "Collecting pymorphy2\n",
      "  Downloading pymorphy2-0.9.1-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting dawg-python>=0.7.1 (from pymorphy2)\n",
      "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting pymorphy2-dicts-ru<3.0,>=2.4 (from pymorphy2)\n",
      "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting docopt>=0.6 (from pymorphy2)\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hDownloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n",
      "Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
      "Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: docopt\n",
      "  Building wheel for docopt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13705 sha256=8aa84ff0b2080bd04907e4b5719151556deade8e340029e69313045a286a53b3\n",
      "  Stored in directory: /home/angelika/snap/jupyterlab-desktop/common/.cache/pip/wheels/1a/bf/a1/4cee4f7678c68c5875ca89eaccf460593539805c3906722228\n",
      "Successfully built docopt\n",
      "Installing collected packages: pymorphy2-dicts-ru, docopt, dawg-python, pymorphy2\n",
      "Successfully installed dawg-python-0.7.2 docopt-0.6.2 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844\n"
     ]
    }
   ],
   "source": [
    "# скачиваем данные и доставляем необходимые библиотеки\n",
    "!wget https://raw.githubusercontent.com/hse-ds/iad-applied-ds/master/2020/seminars/seminar11/alice.txt -N\n",
    "!wget https://raw.githubusercontent.com/hse-ds/iad-applied-ds/master/2020/seminars/seminar11/dinos.txt -N\n",
    "!pip install nltk\n",
    "!pip install pymystem3==0.1.10\n",
    "!pip install pymorphy2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Задание 9\n",
    "\n",
    "Откройте и запишите в одну текстовую строку файл alice.txt.\n",
    "\n",
    "Ответьте на вопросы:\n",
    "\n",
    "    Сколько всего символов в файле?\n",
    "    Какой текст написан в последней строке файла? (Подсказка: в какой переменной содержится эта строка?)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129385\n"
     ]
    }
   ],
   "source": [
    "with open('alice.txt', 'r', encoding='utf-8') as file:\n",
    "    content = file.read()\n",
    "    total_characters = len(content)\n",
    "\n",
    "print(total_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество строк:3147, last_line: И, наконец, она представила себе, как ее маленькая сестра вырастет и, сохранив в свои зрелые годы простое и любящее детское сердце, станет собирать вокруг себя других детей, и как их глаза заблестят от дивных сказок. Быть может, она поведает им и о Стране Чудес и, разделив с ними их нехитрые горести и нехитрые радости, вспомнит свое детство и счастливые летние дни.\n",
      " \n"
     ]
    }
   ],
   "source": [
    "with open('alice.txt', 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "    len_ = len(lines)\n",
    "\n",
    "last_line = lines[-1]\n",
    "\n",
    "print(f'Количество строк:{len_}, last_line: {last_line}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Задание 10\n",
    "\n",
    "Создайте и скомпилируйте регулярное выражения для разбиения полученной строки на токены. Не забудьте про дефис и букву ё.\n",
    "\n",
    "Найдите все токены с помощью этого регулярного выражения, ответьте на вопросы:\n",
    "\n",
    "    Сколько токенов получилось?\n",
    "    Какой токен первый?\n",
    "    Какой токен последний?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество токенов: 61\n",
      "Первый токен: И\n",
      "Последний токен: дни\n"
     ]
    }
   ],
   "source": [
    "pattern = r'[А-Яа-яёЁ\\-]+'\n",
    "\n",
    "# Находим все токены\n",
    "tokens = re.findall(pattern, last_line)\n",
    "\n",
    "# Выводим результаты\n",
    "print(f\"Количество токенов: {len(tokens)}\")\n",
    "print(f\"Первый токен: {tokens[0]}\")\n",
    "print(f\"Последний токен: {tokens[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Токенизация в nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/angelika/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "#nltk.download('punkt') — команда для загрузки предварительно \n",
    "#обученного токенизатора, который используется \n",
    "#функцией word_tokenize. punkt — это набор данных для токенизации,\n",
    "#включающий модели для различных языков, \n",
    "#которые помогают правильно разделять текст на слова и знаки препинания."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Но', 'не', 'каждый', 'хочет', 'что-то', 'исправлять', ':', '(']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = 'Но не каждый хочет что-то исправлять:('\n",
    "word_tokenize(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BlanklineTokenizer',\n",
       " 'LegalitySyllableTokenizer',\n",
       " 'LineTokenizer',\n",
       " 'MWETokenizer',\n",
       " 'NLTKWordTokenizer',\n",
       " 'PunktSentenceTokenizer',\n",
       " 'RegexpTokenizer',\n",
       " 'ReppTokenizer',\n",
       " 'SExprTokenizer',\n",
       " 'SpaceTokenizer',\n",
       " 'StanfordSegmenter',\n",
       " 'SyllableTokenizer',\n",
       " 'TabTokenizer',\n",
       " 'TextTilingTokenizer',\n",
       " 'ToktokTokenizer',\n",
       " 'TreebankWordDetokenizer']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import tokenize\n",
    "dir(tokenize)[:16]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Они умеют выдавать индексы начала и конца каждого токена:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 2), (3, 5), (6, 12), (13, 18), (19, 25), (26, 38)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wh_tok = tokenize.WhitespaceTokenizer()\n",
    "list(wh_tok.span_tokenize(example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(если вам было интересно, зачем вообще включать в модуль токенизатор, который работает как .split() :))\n",
    "\n",
    "Некторые токенизаторы ведут себя специфично:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['do', \"n't\", 'stop', 'me']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize.TreebankWordTokenizer().tokenize(\"don't stop me\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для некоторых задач это может быть полезно.\n",
    "\n",
    "А некоторые -- вообще не для текста на естественном языке (не очень понятно, зачем это в nltk :)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['(a (b c))', 'd', 'e', '(f)']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize.SExprTokenizer().tokenize(\"(a (b c)) d e (f)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Лемматизация\n",
    "\n",
    "Лемматизация – это сведение разных форм одного слова к начальной форме – лемме. Почему это хорошо?\n",
    "\n",
    "    Во-первых, мы хотим рассматривать как отдельную фичу каждое слово, а не каждую его отдельную форму.\n",
    "    Во-вторых, некоторые стоп-слова стоят только в начальной форме, и без лематизации выкидываем мы только её.\n",
    "\n",
    "Для русского есть два хороших лемматизатора: mystem и pymorphy:\n",
    "\n",
    "\n",
    "### Mystem\n",
    "\n",
    "https://yandex.ru/dev/mystem/doc/index-docpage/\n",
    "\n",
    "Как с ним работать:\n",
    "* можно скачать mystem и запускать из терминала с разными параметрами\n",
    "* pymystem3 - обертка для питона, работает медленнее, но это удобно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Installing mystem to /home/angelika/.local/bin/mystem from http://download.cdn.yandex.net/mystem/mystem-3.0-linux3.1-64bit.tar.gz\n"
     ]
    }
   ],
   "source": [
    "from pymystem3 import Mystem\n",
    "mystem_analyzer = Mystem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы инициализировали Mystem c дефолтными параметрами. А вообще параметры есть такие:\n",
    "\n",
    "    mystem_bin - путь к mystem, если их несколько\n",
    "    grammar_info - нужна ли грамматическая информация или только леммы (по дефолту нужна)\n",
    "    disambiguation - нужно ли снятие омонимии - дизамбигуация (по дефолту нужна)\n",
    "    entire_input - нужно ли сохранять в выводе все (пробелы всякие, например), или можно выкинуть (по дефолту оставляется все)\n",
    "\n",
    "Методы Mystem принимают строку, токенизатор вшит внутри. Можно, конечно, и пословно анализировать, но тогда он не сможет учитывать контекст.\n",
    "\n",
    "Можно просто лемматизировать текст:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Но не каждый хочет что-то исправлять:(\n",
      "['но', ' ', 'не', ' ', 'каждый', ' ', 'хотеть', ' ', 'что-то', ' ', 'исправлять', ':(\\n']\n"
     ]
    }
   ],
   "source": [
    "print(example)\n",
    "print(mystem_analyzer.lemmatize(example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А можно получить грамматическую информацию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'analysis': [{'lex': 'но', 'wt': 0.9998906255, 'gr': 'CONJ='}],\n",
       "  'text': 'Но'},\n",
       " {'text': ' '},\n",
       " {'analysis': [{'lex': 'не', 'wt': 1, 'gr': 'PART='}], 'text': 'не'},\n",
       " {'text': ' '},\n",
       " {'analysis': [{'lex': 'каждый',\n",
       "    'wt': 0.9985975623,\n",
       "    'gr': 'APRO=(вин,ед,муж,неод|им,ед,муж)'}],\n",
       "  'text': 'каждый'},\n",
       " {'text': ' '},\n",
       " {'analysis': [{'lex': 'хотеть',\n",
       "    'wt': 1,\n",
       "    'gr': 'V,несов,пе=непрош,ед,изъяв,3-л'}],\n",
       "  'text': 'хочет'},\n",
       " {'text': ' '},\n",
       " {'analysis': [{'lex': 'что-то', 'wt': 1, 'gr': 'SPRO,ед,сред,неод=(вин|им)'}],\n",
       "  'text': 'что-то'},\n",
       " {'text': ' '},\n",
       " {'analysis': [{'lex': 'исправлять', 'wt': 1, 'gr': 'V,пе=инф,несов'}],\n",
       "  'text': 'исправлять'},\n",
       " {'text': ':(\\n'}]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mystem_analyzer.analyze(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Стоп-слова и пунктуация\n",
    "\n",
    "Стоп-слова -- это слова, которые часто встречаются практически в любом тексте и ничего интересного не говорят о конретном документе, то есть играют роль шума. Поэтому их принято убирать. По той же причине убирают и пунктуацию.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/angelika/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['и',\n",
       " 'в',\n",
       " 'во',\n",
       " 'не',\n",
       " 'что',\n",
       " 'он',\n",
       " 'на',\n",
       " 'я',\n",
       " 'с',\n",
       " 'со',\n",
       " 'как',\n",
       " 'а',\n",
       " 'то',\n",
       " 'все',\n",
       " 'она',\n",
       " 'так',\n",
       " 'его',\n",
       " 'но',\n",
       " 'да',\n",
       " 'ты',\n",
       " 'к',\n",
       " 'у',\n",
       " 'же',\n",
       " 'вы',\n",
       " 'за',\n",
       " 'бы',\n",
       " 'по',\n",
       " 'только',\n",
       " 'ее',\n",
       " 'мне',\n",
       " 'было',\n",
       " 'вот',\n",
       " 'от',\n",
       " 'меня',\n",
       " 'еще',\n",
       " 'нет',\n",
       " 'о',\n",
       " 'из',\n",
       " 'ему',\n",
       " 'теперь',\n",
       " 'когда',\n",
       " 'даже',\n",
       " 'ну',\n",
       " 'вдруг',\n",
       " 'ли',\n",
       " 'если',\n",
       " 'уже',\n",
       " 'или',\n",
       " 'ни',\n",
       " 'быть',\n",
       " 'был',\n",
       " 'него',\n",
       " 'до',\n",
       " 'вас',\n",
       " 'нибудь',\n",
       " 'опять',\n",
       " 'уж',\n",
       " 'вам',\n",
       " 'ведь',\n",
       " 'там',\n",
       " 'потом',\n",
       " 'себя',\n",
       " 'ничего',\n",
       " 'ей',\n",
       " 'может',\n",
       " 'они',\n",
       " 'тут',\n",
       " 'где',\n",
       " 'есть',\n",
       " 'надо',\n",
       " 'ней',\n",
       " 'для',\n",
       " 'мы',\n",
       " 'тебя',\n",
       " 'их',\n",
       " 'чем',\n",
       " 'была',\n",
       " 'сам',\n",
       " 'чтоб',\n",
       " 'без',\n",
       " 'будто',\n",
       " 'чего',\n",
       " 'раз',\n",
       " 'тоже',\n",
       " 'себе',\n",
       " 'под',\n",
       " 'будет',\n",
       " 'ж',\n",
       " 'тогда',\n",
       " 'кто',\n",
       " 'этот',\n",
       " 'того',\n",
       " 'потому',\n",
       " 'этого',\n",
       " 'какой',\n",
       " 'совсем',\n",
       " 'ним',\n",
       " 'здесь',\n",
       " 'этом',\n",
       " 'один',\n",
       " 'почти',\n",
       " 'мой',\n",
       " 'тем',\n",
       " 'чтобы',\n",
       " 'нее',\n",
       " 'сейчас',\n",
       " 'были',\n",
       " 'куда',\n",
       " 'зачем',\n",
       " 'всех',\n",
       " 'никогда',\n",
       " 'можно',\n",
       " 'при',\n",
       " 'наконец',\n",
       " 'два',\n",
       " 'об',\n",
       " 'другой',\n",
       " 'хоть',\n",
       " 'после',\n",
       " 'над',\n",
       " 'больше',\n",
       " 'тот',\n",
       " 'через',\n",
       " 'эти',\n",
       " 'нас',\n",
       " 'про',\n",
       " 'всего',\n",
       " 'них',\n",
       " 'какая',\n",
       " 'много',\n",
       " 'разве',\n",
       " 'три',\n",
       " 'эту',\n",
       " 'моя',\n",
       " 'впрочем',\n",
       " 'хорошо',\n",
       " 'свою',\n",
       " 'этой',\n",
       " 'перед',\n",
       " 'иногда',\n",
       " 'лучше',\n",
       " 'чуть',\n",
       " 'том',\n",
       " 'нельзя',\n",
       " 'такой',\n",
       " 'им',\n",
       " 'более',\n",
       " 'всегда',\n",
       " 'конечно',\n",
       " 'всю',\n",
       " 'между']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stopwords.words('russian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from string import punctuation\n",
    "punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуем функцию для препроцессинга текста, будем удалять стоп слова и пунктуацию, а в качестве токенизатора и лемматизатора воспользуемся майстемом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['каждый', 'хотеть', 'чтото', 'исправлять']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def my_preproc_mystem(text):\n",
    "    text = re.sub('[{}]'.format(punctuation), '', text)\n",
    "    text = mystem_analyzer.lemmatize(text)\n",
    "    return [word for word in text if word not in stopwords.words('russian') + [' ', '\\n']]\n",
    "\n",
    "print(my_preproc_mystem(example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pymorphy\n",
    "\n",
    "Это модуль на питоне, довольно быстрый и с кучей функций.\n",
    "\n",
    "https://pymorphy2.readthedocs.io/en/latest/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-09-10 13:49:05--  https://github.com/no-plagiarism/pymorphy3.git\n",
      "Resolving github.com (github.com)... 140.82.121.3\n",
      "Connecting to github.com (github.com)|140.82.121.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: https://github.com/no-plagiarism/pymorphy3 [following]\n",
      "--2024-09-10 13:49:05--  https://github.com/no-plagiarism/pymorphy3\n",
      "Reusing existing connection to github.com:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [text/html]\n",
      "Saving to: 'pymorphy3.git'\n",
      "\n",
      "pymorphy3.git           [  <=>               ] 324.63K  1.47MB/s    in 0.2s    \n",
      "\n",
      "2024-09-10 13:49:06 (1.47 MB/s) - 'pymorphy3.git' saved [332423]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#from pymorphy2 import MorphAnalyzer\n",
    "#pymorphy2_analyzer = MorphAnalyzer()\n",
    "\n",
    "\n",
    "#tldr: Проект pymorphy2 заброшен автором. \n",
    "#Вместо него на данный момент можно использовать \n",
    "#его форк pymorphy3, в нем есть поддержка Python 3.11 и 3.12.\n",
    "# https://github.com/no-plagiarism/pymorphy3.git\n",
    "\n",
    "\n",
    "!wget https://github.com/no-plagiarism/pymorphy3.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pymorphy2 работает с отдельными словами. Если дать ему на вход предложение - он его просто не лемматизирует, т.к. не понимает"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ana = pymorphy2_analyzer.parse(example)\n",
    "#ana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ana[0].normal_form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 11. Реализуйте функцию ```my_preproc_pymorphy``` по аналогии с ```my_preproc_mystem```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_preproc_pymorphy(text):\n",
    "    ####### Ваш код здесь ########\n",
    "    raise NotImplementedError\n",
    "    ##############################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравним результаты:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Mystem: {my_preproc_mystem(example)}')\n",
    "print(f'PyMorphy: {my_preproc_pymorphy(example)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Частотность слов можно посмотреть, с помощью ```nltk.FreqDist```. Выведем топ 20 самых популярных токенов до и  после обработки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "tokenized = word_tokenize(s)\n",
    "alice_d_tokenized = nltk.FreqDist(tokenized)\n",
    "alice_d_tokenized.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "alice_preproc = my_preproc_pymorphy(s)\n",
    "\n",
    "alice_d = nltk.FreqDist(alice_preproc)\n",
    "alice_d.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 12. Сколько раз встречается слово встречалось слово 'крокет' до и после обработки? Найдите изначальные слова, используя ```re.findall()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# разберемся, если нужно будет "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mystem vs. Pymorphy\n",
    "\n",
    "1) Mystem работает невероятно медленно под windows на больших текстах.\n",
    "\n",
    "2) Снятие омонимии. Mystem умеет снимать омонимию по контексту (хотя не всегда преуспевает), pymorphy2 берет на вход одно слово и соответственно вообще не умеет дизамбигуировать по контексту:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "homonym1 = 'За время обучения я прослушал больше сорока курсов.'\n",
    "homonym2 = 'Сорока своровала блестящее украшение со стола.'\n",
    "mystem_analyzer = Mystem() # инициализирую объект с дефолтными параметрами\n",
    "\n",
    "print(mystem_analyzer.analyze(homonym1)[-5])\n",
    "print(mystem_analyzer.analyze(homonym2)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Стэммминг\n",
    "\n",
    "Для русского языка можно воспользоваться SnowballStemmer из пакета nltk.stem    \n",
    "\n",
    "Стемминг – это грубый эвристический процесс, который отрезает «лишнее» от корня слов, часто это приводит к потере словообразовательных суффиксов. Лемматизация – это более тонкий процесс, который использует словарь и морфологический анализ, чтобы в итоге привести слово к его канонической форме – лемме.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer(\"russian\")\n",
    "\n",
    "plurals = ['мухи', \"умирает\", \"мулы\", \"отказано\", \"умер\", \n",
    "           \"согласился\", \"владел\", \"унижен\", \"измерен\", \"встреча\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['мух',\n",
       " 'умира',\n",
       " 'мул',\n",
       " 'отказа',\n",
       " 'умер',\n",
       " 'соглас',\n",
       " 'владел',\n",
       " 'униж',\n",
       " 'измер',\n",
       " 'встреч']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "singles = [stemmer.stem(plural) for plural in plurals]\n",
    "singles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Языковые модели. Модель биграм\n",
    "\n",
    "Какое слово в последовательности вероятнее: \n",
    "\n",
    "Поезд прибыл на\n",
    "* вокзал\n",
    "* север\n",
    "\n",
    "Какая последовательность вероятнее:\n",
    "* Вокзал прибыл поезд на\n",
    "* Поезд прибыл на вокзал\n",
    "\n",
    "### Приложения:\n",
    "* Задачи, в которых нужно обработать сложный и зашумленный вход: распознавание речи, распознавание сканированных и рукописных текстов;\n",
    "* Исправление опечаток\n",
    "* Машинный перевод\n",
    "* Подсказка при наборе \n",
    "\n",
    "### Виды моделей:\n",
    "* Счетные модели\n",
    "    * цепи Маркова\n",
    "* Нейронные модели, обычно реккурентные нейронные сети с LSTM/GRU\n",
    "* seq2seq архитектуры\n",
    "\n",
    "### Модель n-gram:\n",
    "\n",
    "Пусть $w_{1:n}=w_1,\\ldots,w_m$ – последовательность слов.\n",
    "\n",
    "Цепное правило:\n",
    "$ P(w_{1:m}) = P(w_1) P(w_2 | w_1) P(w_3 | w_{1:2}) \\ldots P(w_m | w_{1:m-1}) = \\prod_{k=1}^{m} P(w_k | w_{1:k-1}) $\n",
    "\n",
    "Но оценить $P(w_k | w_{1:k-1})$ не легче! \n",
    "\n",
    "Переходим к $n$-грамам: $P(w_{i+1} | w_{1:i}) \\approx P(w_{i+1} | w_{i-n:i})  $ , то есть, учитываем $n-1$ предыдущее слово. \n",
    "\n",
    "Модель\n",
    "* униграм: $P(w_k)$\n",
    "* биграм: $P(w_k | w_{k-1})$\n",
    "* триграм: $P(w_k | w_{k-1} w_{k-2})$\n",
    "\n",
    "\n",
    "Т.е. используем Марковские допущения о длине запоминаемой цепочки.\n",
    "\n",
    "* Вероятность следующего слова в последовательности: $ P(w_{i+1} | w_{1:i}) \\approx P(w_{i-n:i}) $\n",
    "* Вероятность всей последовательности слов: $P(w_{1:n}) = \\prod_{k=1}^l P(w_k | w_{k-n+1: k-1}) $\n",
    "\n",
    "### Качество модели  $n$-грам\n",
    "\n",
    "Перплексия: насколько хорошо модель предсказывает выборку. Чем ниже значение перплексии, тем лучше.\n",
    "\n",
    "$PP(\\texttt{LM}) = 2 ^ {-\\frac{1}{m} \\log_2 \\texttt{LM} (w_i | w_{1:i-1})}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Рассмотрим модель биграм, используя библиотеку NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aachenosaurus', 'aardonyx', 'abdallahsaurus', 'abelisaurus', 'abrictosaurus', 'abrosaurus', 'abydosaurus', 'acanthopholis', 'achelousaurus', 'acheroraptor']\n"
     ]
    }
   ],
   "source": [
    "with open(\"dinos.txt\") as f:\n",
    "    names = [name.strip().lower() for name in f.readlines()]\n",
    "    print(names[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получить информацию о количестве символов можно, используя ```nltk.FreqDist```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'a': 2487, 's': 2285, 'u': 2123, 'o': 1710, 'r': 1704, 'n': 1081, 'i': 944, 'e': 913, 't': 852, 'l': 617, ...})"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = [char  for name in names for char in name]\n",
    "freq = nltk.FreqDist(chars)\n",
    "\n",
    "freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наиболее вероятный следующий токен можно получить с помощью  ```nltk.ConditionalFreqDist``` и ```nltk.ConditionalProbDist```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'u': 792, 'n': 354, 't': 213, 's': 187, 'l': 146, 'r': 131, 'c': 109, 'p': 96, 'm': 74, 'e': 48, ...})"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfreq = nltk.ConditionalFreqDist(nltk.bigrams(chars))\n",
    "cfreq['a']\n",
    "\n",
    "# Строка 2: cfreq['a']\n",
    "# Здесь используется доступ к условному частотному распределению для символа 'a'.\n",
    "# cfreq['a'] возвращает частотное распределение всех символов, которые следуют за символом 'a' в списке chars. \n",
    "# Это объект типа FreqDist, который показывает, какие символы идут после 'a' и сколько раз каждый из них встречается."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p(a,u) = 0.3185\n"
     ]
    }
   ],
   "source": [
    "cprob = nltk.ConditionalProbDist(cfreq, nltk.MLEProbDist)\n",
    "print('p(a,u) = %1.4f' % cprob['a'].prob('u'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно порождать случайные символы с учётом предыдущих:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'l'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cprob['a'].generate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 13. Напишите функцию для оценки вероятности имени динозавра и найдите наиболее вероятное имя из загруженного списка."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Наиболее вероятное имя: mei\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from functools import reduce\n",
    "\n",
    "with open(\"dinos.txt\") as f:\n",
    "    names = [name.strip().lower() for name in f.readlines()]\n",
    "\n",
    "# Symbol frequency\n",
    "chars = [char for name in names for char in name]\n",
    "freq = nltk.FreqDist(chars)\n",
    "\n",
    "# total number of characters to normalize probabilities\n",
    "total_chars = sum(freq.values())\n",
    "\n",
    "def score_probability(name, freq, total_chars):\n",
    "    prob = reduce(lambda x, y: x * y, \n",
    "                  [(freq[char] / total_chars) for char in name], 1)\n",
    "    return prob\n",
    "\n",
    "# Поиск имени с наибольшей вероятностью\n",
    "most_probable_name = max(names, key=lambda name:  score_probability(name, freq, total_chars))\n",
    "\n",
    "\n",
    "# Вывод наиболее вероятного имени\n",
    "print(f\"Наиболее вероятное имя: {most_probable_name}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
