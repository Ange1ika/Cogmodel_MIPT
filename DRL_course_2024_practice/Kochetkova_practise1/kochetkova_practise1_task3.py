# -*- coding: utf-8 -*-
"""Kochetkova_practise1_task3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MmWIxC1ezI09Lp9tcztFtQI_rUJsbLCD
"""

import numpy as np
import matplotlib.pyplot as plt
import gym
import time

class CrossEntropyAgent():
    def __init__(self, state_n, action_n):
        self.state_n = state_n
        self.action_n = action_n
        self.model = np.ones((self.state_n, self.action_n)) / self.action_n

    def get_action(self, state):
        action = np.random.choice(np.arange(self.action_n), p=self.model[state])
        return int(action)

    def fit_policy_smoothing(self, elite_trajectories, lambda_):
        new_model = np.zeros((self.state_n, self.action_n))
        for trajectory in elite_trajectories:
            for state, action in zip(trajectory['states'], trajectory['actions']):
                new_model[state][action] += 1

        for state in range(self.state_n):
            if np.sum(new_model[state]) > 0:
                new_model[state] /= np.sum(new_model[state])
            else:
                new_model[state] = self.model[state].copy()

        # Policy smoothing with lambda
        self.model = lambda_ * new_model + (1 - lambda_) * self.model

def get_trajectory(env, agent, max_len=1000, visualise=False):
    trajectory = {'states': [], 'actions': [], 'rewards': []}
    obs = env.reset()
    state = obs

    for _ in range(max_len):
        trajectory['states'].append(state)
        action = agent.get_action(state)
        trajectory['actions'].append(action)

        obs, reward, done, _ = env.step(action)
        trajectory['rewards'].append(reward)
        state = obs

        if visualise:
            time.sleep(0.5)
            env.render()

        if done:
            break
    return trajectory

def evaluate_deterministic_policy(env, agent, M, K):
    """
    Генерация детерминированных политик и их оценка.
    """
    deterministic_policies = []
    V_policies = []

    for m in range(M):
        policy = np.zeros((agent.state_n, agent.action_n))
        for state in range(agent.state_n):
            action = np.random.choice(np.arange(agent.action_n), p=agent.model[state])
            policy[state, action] = 1
        deterministic_policies.append(policy)

        total_rewards = []
        for _ in range(K):
            trajectory = get_trajectory(env, agent)
            total_rewards.append(np.sum(trajectory['rewards']))

        V_policies.append(np.mean(total_rewards))

    return deterministic_policies, V_policies

# Основные параметры
M = 5  # Количество детерминированных политик
K = 10  # Количество траекторий для каждой политики
lambda_ = 0.1  # Параметр сглаживания
iteration_n = 20
trajectory_n = 100  # Количество траекторий для каждого итерации

env = gym.make('Taxi-v3')
action_n = env.action_space.n
state_n = env.observation_space.n
agent_policy_smoothing = CrossEntropyAgent(state_n, action_n)

q_param = 0.9  # Квантиль для отбора элитных траекторий
mean_rewards_policy_smoothing = []

for iteration in range(iteration_n):
    # Генерация детерминированных политик и оценка
    deterministic_policies, V_policies = evaluate_deterministic_policy(env, agent_policy_smoothing, M, K)

    # Определение q-квантили наград
    quantile_value = np.quantile(V_policies, q_param)

    # Отбор элитных траекторий
    elite_trajectories = []
    for m in range(M):
        if V_policies[m] > quantile_value:
            for _ in range(K):
                trajectory = get_trajectory(env, agent_policy_smoothing)
                elite_trajectories.append(trajectory)

    # Policy improvement
    agent_policy_smoothing.fit_policy_smoothing(elite_trajectories, lambda_)

    # Оценка средней награды за итерацию
    total_rewards = [np.sum(trajectory['rewards']) for trajectory in elite_trajectories]
    mean_reward = np.mean(total_rewards)
    mean_rewards_policy_smoothing.append(mean_reward)

    print(f"Iteration {iteration + 1}/{iteration_n}, Mean Reward: {mean_reward}")

# График изменения средней награды
plt.figure(figsize=(8, 6))
plt.plot(range(iteration_n), mean_rewards_policy_smoothing, label='Policy Smoothing')
plt.xlabel('Iteration')
plt.ylabel('Mean Total Reward')
plt.title('Mean Total Reward over Iterations')
plt.legend()
plt.show()